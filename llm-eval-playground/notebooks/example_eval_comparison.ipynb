{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Eval Playground: GPT-4 vs Mistral\n",
    "\n",
    "In this notebook, we evaluate outputs from two LLMs (GPT-4 and Mistral) on three prompts.\n",
    "\n",
    "Evaluation is done using the [LLM Evaluation Toolkit](https://github.com/epaunova/LLM-Evaluation-Toolkit), which scores each output on factuality, clarity, and verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install / Import dependencies\n",
    "!pip install openai pandas matplotlib\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define your prompts\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Summarize the latest IPCC climate report.\",\n",
    "    \"What are the pros and cons of remote work?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (Mocked) Example responses for each model\n",
    "responses = {\n",
    "    \"gpt-4\": [\n",
    "        \"Quantum computing uses quantum bits...\",\n",
    "        \"The IPCC report says global warming is accelerating...\",\n",
    "        \"Remote work offers flexibility but reduces team cohesion...\"\n",
    "    ],\n",
    "    \"mistral\": [\n",
    "        \"Quantum computers rely on qubits and superposition...\",\n",
    "        \"Climate change is progressing rapidly, warns IPCC...\",\n",
    "        \"Working remotely improves productivity but creates isolation...\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Example evaluation scores (mocked)\n",
    "data = {\n",
    "    'Prompt': prompts * 2,\n",
    "    'Model': ['gpt-4']*3 + ['mistral']*3,\n",
    "    'Factuality': [0.95, 0.92, 0.88, 0.91, 0.87, 0.86],\n",
    "    'Clarity': [0.97, 0.94, 0.91, 0.92, 0.89, 0.88],\n",
    "    'Verbosity': [0.85, 0.88, 0.82, 0.84, 0.83, 0.81],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize evaluation comparison\n",
    "metrics = ['Factuality', 'Clarity', 'Verbosity']\n",
    "\n",
    "for metric in metrics:\n",
    "    df.groupby('Model')[metric].mean().plot(kind='bar', title=f'Average {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
