# LLM Eval Playground

This project contains notebooks and tools for evaluating LLM responses using structured scoring rubrics.

## Structure
- `notebooks/`: Jupyter Notebooks for running evaluation experiments
- `data/`: Sample prompts and responses
- `outputs/`: Evaluation results
- `utils/`: Helper scripts
