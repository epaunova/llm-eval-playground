# ğŸ§ª LLM Eval Playground

This project demonstrates how to evaluate and compare outputs from different Large Language Models (LLMs) using structured scoring methods â€” including factuality, clarity, and verbosity.

It complements my [LLM Evaluation Toolkit](https://github.com/epaunova/LLM-Evaluation-Toolkit), which provides a configurable scoring pipeline powered by GPT-based auto-grading.

---

## ğŸ¯ Objective

Help Product Managers, ML engineers, and researchers:

- Compare LLM responses across models and prompts
- Visualize differences in output quality
- Experiment with evaluation strategies in real-world tasks

---

## ğŸ“‚ Structure

llm-eval-playground/
â”œâ”€â”€ notebooks/ # Jupyter Notebooks for experimentation
â”‚ â””â”€â”€ example_eval_comparison.ipynb
â”œâ”€â”€ data/ # Sample prompts and model responses
â”œâ”€â”€ outputs/ # Evaluation results and visual charts
â”œâ”€â”€ utils/ # Optional helper scripts
â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸ““ Example Notebook

ğŸ“ [`example_eval_comparison.ipynb`](notebooks/example_eval_comparison.ipynb)

In this notebook, we:

- Compare GPT-4 vs Mistral responses on 3 prompts  
- Score outputs on:
  - âœ… Factuality  
  - âœ… Clarity  
  - âœ… Verbosity
- Visualize results using `matplotlib`

ğŸ§ª *Note: This notebook uses mock data to demonstrate evaluation structure and visual analysis.*

---

## ğŸ“Š Sample Charts

Below are comparisons between GPT-4 and Mistral across three evaluation metrics:

![Factuality](https://github.com/epaunova/llm-eval-playground/raw/main/outputs/factuality_comparison.png)
![Clarity](https://github.com/epaunova/llm-eval-playground/raw/main/outputs/clarity_comparison.png)
![Verbosity](https://github.com/epaunova/llm-eval-playground/raw/main/outputs/verbosity_comparison.png)


---

## ğŸ”§ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/epaunova/llm-eval-playground.git
   cd llm-eval-playground
Open the notebook:

bash
Copy
Edit
cd notebooks
jupyter notebook example_eval_comparison.ipynb
(Optional) Connect with the LLM Evaluation Toolkit to run real GPT-based scoring.

ğŸŒ Author
ğŸ‘¤ Eva Paunova
ğŸ”— LinkedIn
ğŸ“‚ Portfolio

ğŸš€ Coming Next
Integration with actual GPT evals (via OpenAI API)

More prompt types and output evaluation

Token-level and latency benchmarks
