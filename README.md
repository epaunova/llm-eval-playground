# ðŸ§ª LLM Eval Playground

This project demonstrates how to evaluate and compare outputs from different Large Language Models (LLMs) using structured scoring methods â€” including factuality, clarity, and verbosity.

It complements my [LLM Evaluation Toolkit](https://github.com/epaunova/LLM-Evaluation-Toolkit), which provides a configurable scoring pipeline powered by GPT-based auto-grading.

---

## ðŸŽ¯ Objective

Help Product Managers, ML engineers, and researchers:

- Compare LLM responses across models and prompts
- Visualize differences in output quality
- Experiment with evaluation strategies in real-world tasks

---

## ðŸ“‚ Structure

